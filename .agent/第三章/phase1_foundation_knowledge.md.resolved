# ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è®¾æ–½ä¸æ¨¡å‹ç®¡ç†å±‚ - è¯¦ç»†çŸ¥è¯†æ–‡æ¡£

> **é˜¶æ®µç›®æ ‡**ï¼šå»ºç«‹ç»Ÿä¸€çš„ AI æ¨¡å‹åŠ è½½ã€æ¨ç†ä¸èµ„æºç®¡ç†åŸºç¡€è®¾æ–½,ä¸ºåç»­ä¸‰ä¸ªæ£€æµ‹å™¨(åœºæ™¯å˜åŒ–ã€è¿åŠ¨æ£€æµ‹ã€æ–‡å­—è¯†åˆ«)æä¾›ç¨³å®šé«˜æ•ˆçš„åº•å±‚æ”¯æ’‘ã€‚

---

## ğŸ“š æ ¸å¿ƒçŸ¥è¯†ä½“ç³»

æœ¬é˜¶æ®µæ¶‰åŠä¸‰ä¸ªæ ¸å¿ƒçŸ¥è¯†é¢†åŸŸï¼š

```mermaid
mindmap
  root((åŸºç¡€è®¾æ–½å±‚))
    æ¨ç†æ¡†æ¶é›†æˆ
      ONNX Runtime æ¶æ„
      PaddleInference æ¶æ„
      è·¨å¹³å°éƒ¨ç½²ç­–ç•¥
    æ¨¡å‹ç®¡ç†è®¾è®¡
      å•ä¾‹æ¨¡å¼ä¸çº¿ç¨‹å®‰å…¨
      æ¨¡å‹é¢„çƒ­æœºåˆ¶
      å†…å­˜æ± ä¸ç¼“å­˜ç­–ç•¥
    æ•°æ®è½¬æ¢å·¥å…·
      OpenCV Mat ç»“æ„
      NCHW vs NHWC å¸ƒå±€
      é›¶æ‹·è´ä¼˜åŒ–æŠ€æœ¯
```

---

## ä¸€ã€æ¨ç†æ¡†æ¶æ·±åº¦è§£æ

### 1.1 ONNX Runtime æ ¸å¿ƒæ¶æ„

**ONNX (Open Neural Network Exchange)** æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ·±åº¦å­¦ä¹ æ¨¡å‹äº¤æ¢æ ¼å¼,è€Œ **ONNX Runtime** æ˜¯å¾®è½¯å¼€å‘çš„é«˜æ€§èƒ½æ¨ç†å¼•æ“ã€‚

#### æ¶æ„å±‚æ¬¡

```mermaid
flowchart TB
    subgraph Application[åº”ç”¨å±‚]
        App[C++ åº”ç”¨ç¨‹åº]
    end
    
    subgraph API[API å±‚]
        Session[Ort::Session]
        Env[Ort::Env]
        MemInfo[Ort::MemoryInfo]
    end
    
    subgraph Core[æ ¸å¿ƒå±‚]
        Graph[è®¡ç®—å›¾ä¼˜åŒ–å™¨]
        Executor[æ‰§è¡Œå¼•æ“]
        Allocator[å†…å­˜åˆ†é…å™¨]
    end
    
    subgraph Backend[åç«¯å±‚]
        CPU[CPU Provider]
        CUDA[CUDA Provider]
        TensorRT[TensorRT Provider]
    end
    
    App --> Session
    App --> Env
    Session --> Graph
    Graph --> Executor
    Executor --> CPU & CUDA & TensorRT
    
    style Application fill:#e3f2fd
    style Backend fill:#fff3e0
```

#### æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

##### 1.1.1 Ort::Env - å…¨å±€ç¯å¢ƒç®¡ç†å™¨

`Ort::Env` æ˜¯ ONNX Runtime çš„å…¨å±€å•ä¾‹,è´Ÿè´£ï¼š
- æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–
- çº¿ç¨‹æ± é…ç½®
- å…¨å±€èµ„æºç®¡ç†

```cpp
// åˆ›å»ºå…¨å±€ç¯å¢ƒ(å•ä¾‹æ¨¡å¼)
Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "KeyFrameSelector");

// æ—¥å¿—çº§åˆ«è¯´æ˜:
// ORT_LOGGING_LEVEL_VERBOSE  - è¯¦ç»†è°ƒè¯•ä¿¡æ¯
// ORT_LOGGING_LEVEL_INFO     - ä¸€èˆ¬ä¿¡æ¯
// ORT_LOGGING_LEVEL_WARNING  - è­¦å‘Š(æ¨è)
// ORT_LOGGING_LEVEL_ERROR    - ä»…é”™è¯¯
```

> [!IMPORTANT]
> `Ort::Env` å¿…é¡»åœ¨æ•´ä¸ªåº”ç”¨ç”Ÿå‘½å‘¨æœŸå†…ä¿æŒå­˜æ´»,é€šå¸¸ä½œä¸º `ModelManager` çš„æˆå‘˜å˜é‡ã€‚

##### 1.1.2 Ort::SessionOptions - ä¼šè¯é…ç½®

æ§åˆ¶æ¨¡å‹æ¨ç†çš„å„ç§å‚æ•°ï¼š

```cpp
Ort::SessionOptions sessionOptions;

// 1. çº¿ç¨‹æ•°é…ç½®(å½±å“å•æ¬¡æ¨ç†çš„å¹¶è¡Œåº¦)
sessionOptions.SetIntraOpNumThreads(4);  // ç®—å­å†…å¹¶è¡Œ
sessionOptions.SetInterOpNumThreads(2);  // ç®—å­é—´å¹¶è¡Œ

// 2. å›¾ä¼˜åŒ–çº§åˆ«
sessionOptions.SetGraphOptimizationLevel(
    GraphOptimizationLevel::ORT_ENABLE_ALL
);
// ä¼˜åŒ–çº§åˆ«è¯´æ˜:
// ORT_DISABLE_ALL  - ç¦ç”¨æ‰€æœ‰ä¼˜åŒ–
// ORT_ENABLE_BASIC - åŸºç¡€ä¼˜åŒ–(å¸¸é‡æŠ˜å ã€ç®—å­èåˆ)
// ORT_ENABLE_EXTENDED - æ‰©å±•ä¼˜åŒ–(å¸ƒå±€è½¬æ¢ã€å†…å­˜å¤ç”¨)
// ORT_ENABLE_ALL   - å…¨éƒ¨ä¼˜åŒ–(æ¨è)

// 3. æ‰§è¡Œæ¨¡å¼
sessionOptions.SetExecutionMode(ExecutionMode::ORT_SEQUENTIAL);
// ORT_SEQUENTIAL - é¡ºåºæ‰§è¡Œ(ä½å»¶è¿Ÿ)
// ORT_PARALLEL   - å¹¶è¡Œæ‰§è¡Œ(é«˜åå)
```

##### 1.1.3 Execution Provider - ç¡¬ä»¶åŠ é€Ÿåç«¯

ONNX Runtime æ”¯æŒå¤šç§ç¡¬ä»¶åŠ é€Ÿæ–¹æ¡ˆï¼š

| Provider | ç¡¬ä»¶ | æ€§èƒ½æå‡ | éƒ¨ç½²å¤æ‚åº¦ |
|:--------:|:----:|:--------:|:----------:|
| **CPU** | é€šç”¨ CPU | åŸºå‡† | â­ ç®€å• |
| **CUDA** | NVIDIA GPU | 3-10x | â­â­ ä¸­ç­‰ |
| **TensorRT** | NVIDIA GPU | 5-20x | â­â­â­ å¤æ‚ |
| **DirectML** | Windows GPU | 2-5x | â­â­ ä¸­ç­‰ |

```cpp
// CPU æ¨¡å¼(é»˜è®¤)
Ort::Session session(env, modelPath.c_str(), sessionOptions);

// CUDA åŠ é€Ÿ(éœ€è¦ CUDA Toolkit)
OrtCUDAProviderOptions cudaOptions;
cudaOptions.device_id = 0;  // GPU è®¾å¤‡ ID
sessionOptions.AppendExecutionProvider_CUDA(cudaOptions);
Ort::Session session(env, modelPath.c_str(), sessionOptions);
```

> [!TIP]
> å¯¹äºæœ¬é¡¹ç›®çš„è½»é‡çº§æ¨¡å‹(MobileNetV3-Small ~2.5MB, YOLOv8n ~6.3MB),CPU æ¨¡å¼å·²è¶³å¤Ÿæ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚ä»…åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡è§†é¢‘æˆ–éœ€è¦æè‡´æ€§èƒ½æ—¶æ‰è€ƒè™‘ GPUã€‚

#### 1.1.4 æ¨ç†æµç¨‹è¯¦è§£

å®Œæ•´çš„æ¨ç†è¿‡ç¨‹åŒ…å«äº”ä¸ªæ­¥éª¤ï¼š

```mermaid
sequenceDiagram
    participant App as åº”ç”¨ç¨‹åº
    participant Sess as Ort::Session
    participant Mem as Memory Allocator
    participant Exec as Executor
    participant Backend as CPU/GPU
    
    App->>Mem: 1. åˆ›å»ºè¾“å…¥ Tensor
    Mem-->>App: Ort::Value
    
    App->>Sess: 2. Run(inputNames, inputs, outputNames)
    Sess->>Exec: 3. æ‰§è¡Œè®¡ç®—å›¾
    Exec->>Backend: 4. è°ƒç”¨ç¡¬ä»¶åç«¯
    Backend-->>Exec: è®¡ç®—ç»“æœ
    Exec-->>Sess: è¾“å‡º Tensor
    Sess-->>App: vector<Ort::Value>
    
    App->>App: 5. æå–æ•°æ®(GetTensorData)
```

**ä»£ç å®ç°ç¤ºä¾‹**ï¼š

```cpp
// æ­¥éª¤ 1: å‡†å¤‡è¾“å…¥æ•°æ®
std::vector<float> inputData = preprocessImage(image);  // [1, 3, 224, 224]
std::vector<int64_t> inputShape = {1, 3, 224, 224};

// æ­¥éª¤ 2: åˆ›å»ºè¾“å…¥ Tensor
auto memoryInfo = Ort::MemoryInfo::CreateCpu(
    OrtArenaAllocator, OrtMemTypeDefault
);
Ort::Value inputTensor = Ort::Value::CreateTensor<float>(
    memoryInfo,
    inputData.data(),
    inputData.size(),
    inputShape.data(),
    inputShape.size()
);

// æ­¥éª¤ 3: æ‰§è¡Œæ¨ç†
const char* inputNames[] = {"input"};
const char* outputNames[] = {"output"};
auto outputTensors = session.Run(
    Ort::RunOptions{nullptr},
    inputNames, &inputTensor, 1,
    outputNames, 1
);

// æ­¥éª¤ 4: æå–ç»“æœ
float* outputData = outputTensors[0].GetTensorMutableData<float>();
auto outputShape = outputTensors[0].GetTensorTypeAndShapeInfo().GetShape();
// outputShape: [1, 1280] for MobileNetV3-Small
```

---

### 1.2 PaddleInference æ¶æ„è§£æ

**PaddleInference** æ˜¯ç™¾åº¦é£æ¡¨(PaddlePaddle)çš„æ¨ç†å¼•æ“,ä¸“ä¸º PaddleOCR ç­‰ç™¾åº¦ç”Ÿæ€æ¨¡å‹ä¼˜åŒ–ã€‚

#### æ ¸å¿ƒç»„ä»¶

```mermaid
classDiagram
    class Config {
        +SetModel(modelPath, paramsPath)
        +EnableUseGpu(memory, deviceId)
        +EnableMKLDNN()
        +SetCpuMathLibraryNumThreads(n)
    }
    
    class Predictor {
        +GetInputHandle(name) Tensor*
        +GetOutputHandle(name) Tensor*
        +Run() bool
    }
    
    class Tensor {
        +Reshape(shape)
        +CopyFromCpu(data)
        +CopyToCpu(data)
        +type() DataType
    }
    
    Config --> Predictor : CreatePredictor()
    Predictor --> Tensor : ç®¡ç†è¾“å…¥è¾“å‡º
```

#### ä¸ ONNX Runtime çš„å¯¹æ¯”

| ç‰¹æ€§ | ONNX Runtime | PaddleInference |
|:----:|:------------:|:---------------:|
| **æ¨¡å‹æ ¼å¼** | .onnx | .pdmodel + .pdiparams |
| **ç”Ÿæ€æ”¯æŒ** | é€šç”¨(PyTorch/TF/...) | é£æ¡¨ä¸“ç”¨ |
| **OCR ä¼˜åŒ–** | ä¸€èˆ¬ | â­â­â­ æ·±åº¦ä¼˜åŒ– |
| **éƒ¨ç½²å¤æ‚åº¦** | ä½ | ä¸­ç­‰ |
| **æ–‡æ¡£è´¨é‡** | ä¼˜ç§€ | è‰¯å¥½ |

> [!NOTE]
> æœ¬é¡¹ç›®ä¸­,MobileNetV3 å’Œ YOLOv8n ä½¿ç”¨ ONNX Runtime,è€Œ PaddleOCR PP-OCRv4 ä½¿ç”¨ PaddleInference,å……åˆ†å‘æŒ¥å„è‡ªä¼˜åŠ¿ã€‚

#### PaddleOCR æ¨ç†ç¤ºä¾‹

```cpp
// 1. é…ç½® Predictor
paddle_infer::Config config;
config.SetModel("ppocr_det.pdmodel", "ppocr_det.pdiparams");
config.EnableMKLDNN();  // å¯ç”¨ Intel MKL-DNN åŠ é€Ÿ
config.SetCpuMathLibraryNumThreads(4);

// 2. åˆ›å»º Predictor
auto predictor = paddle_infer::CreatePredictor(config);

// 3. å‡†å¤‡è¾“å…¥
auto inputNames = predictor->GetInputNames();
auto inputTensor = predictor->GetInputHandle(inputNames[0]);
inputTensor->Reshape({1, 3, 640, 640});
inputTensor->CopyFromCpu(imageData.data());

// 4. æ‰§è¡Œæ¨ç†
predictor->Run();

// 5. è·å–è¾“å‡º
auto outputNames = predictor->GetOutputNames();
auto outputTensor = predictor->GetOutputHandle(outputNames[0]);
std::vector<float> outputData(outputTensor->size());
outputTensor->CopyToCpu(outputData.data());
```

---

## äºŒã€ModelManager è®¾è®¡æ¨¡å¼è¯¦è§£

### 2.1 å•ä¾‹æ¨¡å¼ä¸çº¿ç¨‹å®‰å…¨

#### ä¸ºä»€ä¹ˆéœ€è¦å•ä¾‹ï¼Ÿ

1. **èµ„æºå…±äº«**ï¼šå¤šä¸ªæ£€æµ‹å™¨å…±äº«åŒä¸€ç»„æ¨¡å‹,é¿å…é‡å¤åŠ è½½
2. **å†…å­˜ä¼˜åŒ–**ï¼šæ¨¡å‹æ–‡ä»¶é€šå¸¸è¾ƒå¤§(æ€»è®¡ ~23MB),å•ä¾‹å¯èŠ‚çœå†…å­˜
3. **å…¨å±€è®¿é—®**ï¼šä»»ä½•æ¨¡å—éƒ½èƒ½æ–¹ä¾¿åœ°è·å–æ¨¡å‹å®ä¾‹

#### C++11 çº¿ç¨‹å®‰å…¨å•ä¾‹å®ç°

```cpp
class ModelManager {
public:
    // Meyer's Singleton (C++11 ä¿è¯çº¿ç¨‹å®‰å…¨)
    static ModelManager& getInstance() {
        static ModelManager instance;  // å±€éƒ¨é™æ€å˜é‡,é¦–æ¬¡è°ƒç”¨æ—¶åˆå§‹åŒ–
        return instance;
    }
    
    // ç¦ç”¨æ‹·è´å’Œèµ‹å€¼
    ModelManager(const ModelManager&) = delete;
    ModelManager& operator=(const ModelManager&) = delete;
    
private:
    ModelManager() {
        // åˆå§‹åŒ– ONNX Runtime ç¯å¢ƒ
        env_ = std::make_unique<Ort::Env>(
            ORT_LOGGING_LEVEL_WARNING, "ModelManager"
        );
    }
    
    std::unique_ptr<Ort::Env> env_;
    std::unordered_map<std::string, std::unique_ptr<Ort::Session>> sessions_;
    std::mutex mutex_;  // ä¿æŠ¤ sessions_ çš„å¹¶å‘è®¿é—®
};
```

> [!IMPORTANT]
> **çº¿ç¨‹å®‰å…¨ä¿è¯**ï¼š
> - C++11 æ ‡å‡†ä¿è¯å±€éƒ¨é™æ€å˜é‡çš„åˆå§‹åŒ–æ˜¯çº¿ç¨‹å®‰å…¨çš„
> - å¯¹å…±äº«èµ„æº(å¦‚ `sessions_` map)çš„è®¿é—®éœ€è¦é¢å¤–åŠ é”

### 2.2 æ¨¡å‹åŠ è½½ä¸é¢„çƒ­æœºåˆ¶

#### å»¶è¿ŸåŠ è½½ vs é¢„åŠ è½½

| ç­–ç•¥ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|:----:|:----:|:----:|:--------:|
| **å»¶è¿ŸåŠ è½½** | å¯åŠ¨å¿«,æŒ‰éœ€åˆ†é… | é¦–æ¬¡æ¨ç†æ…¢ | æ¨¡å‹å¤šä¸”ä¸å¸¸ç”¨ |
| **é¢„åŠ è½½** | é¦–æ¬¡æ¨ç†å¿«,æ€§èƒ½ç¨³å®š | å¯åŠ¨æ…¢,å†…å­˜å ç”¨é«˜ | æ¨¡å‹å°‘ä¸”å¿…ç”¨ |

**æœ¬é¡¹ç›®é‡‡ç”¨é¢„åŠ è½½ç­–ç•¥**,å› ä¸ºä¸‰ä¸ªæ¨¡å‹éƒ½æ˜¯æ ¸å¿ƒç»„ä»¶,å¿…ç„¶ä¼šè¢«ä½¿ç”¨ã€‚

#### æ¨¡å‹é¢„çƒ­(Warm-up)åŸç†

æ¨¡å‹é¦–æ¬¡æ¨ç†æ—¶,æ¨ç†å¼•æ“éœ€è¦ï¼š
1. åˆ†é…å†…å­˜ç¼“å†²åŒº
2. ä¼˜åŒ–è®¡ç®—å›¾
3. åˆå§‹åŒ– CUDA ä¸Šä¸‹æ–‡(å¦‚æœä½¿ç”¨ GPU)

è¿™äº›æ“ä½œä¼šå¯¼è‡´é¦–æ¬¡æ¨ç†å»¶è¿Ÿè¾ƒé«˜(å¯èƒ½æ˜¯åç»­æ¨ç†çš„ 10-100 å€)ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨åº”ç”¨å¯åŠ¨æ—¶,ç”¨éšæœºæ•°æ®æ‰§è¡Œä¸€æ¬¡"å‡æ¨ç†"ã€‚

```cpp
void ModelManager::warmUpModel(const std::string& modelName) {
    auto it = sessions_.find(modelName);
    if (it == sessions_.end()) {
        throw std::runtime_error("Model not loaded: " + modelName);
    }
    
    auto& session = it->second;
    
    // 1. è·å–è¾“å…¥å½¢çŠ¶
    auto inputShape = session->GetInputTypeInfo(0)
        .GetTensorTypeAndShapeInfo()
        .GetShape();
    
    // 2. åˆ›å»ºéšæœºè¾“å…¥æ•°æ®
    size_t inputSize = 1;
    for (auto dim : inputShape) {
        inputSize *= (dim > 0 ? dim : 1);  // å¤„ç†åŠ¨æ€ç»´åº¦(-1)
    }
    std::vector<float> dummyInput(inputSize, 0.5f);
    
    // 3. æ‰§è¡Œé¢„çƒ­æ¨ç†
    auto memoryInfo = Ort::MemoryInfo::CreateCpu(
        OrtArenaAllocator, OrtMemTypeDefault
    );
    Ort::Value inputTensor = Ort::Value::CreateTensor<float>(
        memoryInfo,
        dummyInput.data(),
        dummyInput.size(),
        inputShape.data(),
        inputShape.size()
    );
    
    const char* inputNames[] = {"input"};
    const char* outputNames[] = {"output"};
    
    // æ‰§è¡Œæ¨ç†(ç»“æœä¼šè¢«ä¸¢å¼ƒ)
    session->Run(
        Ort::RunOptions{nullptr},
        inputNames, &inputTensor, 1,
        outputNames, 1
    );
    
    std::cout << "[ModelManager] Model warmed up: " << modelName << std::endl;
}
```

> [!TIP]
> é¢„çƒ­æ“ä½œåº”åœ¨åº”ç”¨å¯åŠ¨æ—¶çš„åå°çº¿ç¨‹ä¸­æ‰§è¡Œ,é¿å…é˜»å¡ä¸»çº¿ç¨‹ã€‚

### 2.3 å†…å­˜ç®¡ç†ä¸ç¼“å­˜ç­–ç•¥

#### æ¨¡å‹å†…å­˜å ç”¨åˆ†æ

```mermaid
pie
    title æ¨¡å‹å†…å­˜åˆ†å¸ƒ
    "MobileNetV3-Small" : 2.5
    "YOLOv8n" : 6.3
    "PP-OCRv4 Det" : 4.4
    "PP-OCRv4 Rec" : 10.5
```

**æ€»å†…å­˜å ç”¨**ï¼šçº¦ 23.7 MB (æ¨¡å‹æƒé‡) + æ¨ç†ç¼“å†²åŒº(åŠ¨æ€åˆ†é…)

#### æ¨ç†ç¼“å†²åŒºå¤ç”¨

æ¯æ¬¡æ¨ç†éƒ½ä¼šåˆ†é…ä¸´æ—¶å†…å­˜ç”¨äºå­˜å‚¨ä¸­é—´ç»“æœã€‚é¢‘ç¹çš„åˆ†é…/é‡Šæ”¾ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**ä¼˜åŒ–ç­–ç•¥**ï¼šä½¿ç”¨ ONNX Runtime çš„ Arena Allocator

```cpp
Ort::SessionOptions sessionOptions;

// å¯ç”¨ Arena åˆ†é…å™¨(é»˜è®¤å·²å¯ç”¨)
sessionOptions.SetExecutionMode(ExecutionMode::ORT_SEQUENTIAL);

// é…ç½® Arena å‚æ•°
sessionOptions.AddConfigEntry(
    "session.intra_op.allow_spinning", "1"  // å…è®¸è‡ªæ—‹ç­‰å¾…
);
sessionOptions.AddConfigEntry(
    "session.inter_op.allow_spinning", "1"
);
```

**Arena Allocator å·¥ä½œåŸç†**ï¼š

```mermaid
flowchart LR
    subgraph Arena[å†…å­˜æ± ]
        Block1[Block 1\n4KB]
        Block2[Block 2\n4KB]
        Block3[Block 3\n4KB]
    end
    
    Inference1[æ¨ç† 1] -->|ç”³è¯·| Block1
    Inference2[æ¨ç† 2] -->|å¤ç”¨| Block1
    Inference3[æ¨ç† 3] -->|å¤ç”¨| Block1
    
    style Arena fill:#e8f5e9
```

> [!NOTE]
> Arena åœ¨é¦–æ¬¡æ¨ç†åä¼šä¿ç•™å†…å­˜,åç»­æ¨ç†ç›´æ¥å¤ç”¨,é¿å…é¢‘ç¹çš„ malloc/freeã€‚

---

## ä¸‰ã€æ•°æ®è½¬æ¢å·¥å…·è¯¦è§£

### 3.1 OpenCV Mat å†…å­˜å¸ƒå±€

#### Mat æ•°æ®ç»“æ„

```cpp
class cv::Mat {
public:
    int rows;       // å›¾åƒé«˜åº¦
    int cols;       // å›¾åƒå®½åº¦
    int type_;      // æ•°æ®ç±»å‹(CV_8UC3, CV_32FC3 ç­‰)
    uchar* data;    // æŒ‡å‘åƒç´ æ•°æ®çš„æŒ‡é’ˆ
    size_t step;    // æ¯è¡Œçš„å­—èŠ‚æ•°(åŒ…å« padding)
};
```

#### å†…å­˜å¸ƒå±€ç¤ºä¾‹

å¯¹äºä¸€å¼  **3Ã—4 çš„ RGB å›¾åƒ** (CV_8UC3)ï¼š

```
å†…å­˜å¸ƒå±€(HWC - Height, Width, Channel):
[R G B] [R G B] [R G B] [R G B]  <- Row 0
[R G B] [R G B] [R G B] [R G B]  <- Row 1
[R G B] [R G B] [R G B] [R G B]  <- Row 2

è¿ç»­å†…å­˜: R G B R G B R G B R G B R G B R G B ...
```

**å…³é”®ç‰¹æ€§**ï¼š
- **äº¤é”™å­˜å‚¨**ï¼šRGB ä¸‰ä¸ªé€šé“äº¤é”™æ’åˆ—
- **è¡Œä¼˜å…ˆ**ï¼šæŒ‰è¡Œè¿ç»­å­˜å‚¨
- **Padding**ï¼šæŸäº›æƒ…å†µä¸‹è¡Œæœ«å¯èƒ½æœ‰å¡«å……å­—èŠ‚(step > cols * channels)

### 3.2 æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¾“å…¥æ ¼å¼

#### NCHW vs NHWC

æ·±åº¦å­¦ä¹ æ¡†æ¶é€šå¸¸ä½¿ç”¨ä¸¤ç§å¼ é‡å¸ƒå±€ï¼š

| æ ¼å¼ | ç»´åº¦é¡ºåº | æ¡†æ¶ | å†…å­˜å¸ƒå±€ |
|:----:|:--------:|:----:|:--------:|
| **NCHW** | Batch, Channel, Height, Width | PyTorch, ONNX | é€šé“è¿ç»­ |
| **NHWC** | Batch, Height, Width, Channel | TensorFlow | åƒç´ è¿ç»­ |

**NCHW å†…å­˜å¸ƒå±€** (æ¨¡å‹è¾“å…¥æ ¼å¼)ï¼š

```
[1, 3, 224, 224] çš„ Tensor:
[R R R R ... R]  <- æ‰€æœ‰ R é€šé“(224Ã—224 ä¸ªå€¼)
[G G G G ... G]  <- æ‰€æœ‰ G é€šé“
[B B B B ... B]  <- æ‰€æœ‰ B é€šé“
```

> [!IMPORTANT]
> OpenCV Mat æ˜¯ **HWC** æ ¼å¼,è€Œ ONNX æ¨¡å‹é€šå¸¸è¦æ±‚ **NCHW** æ ¼å¼,éœ€è¦è¿›è¡Œè½¬æ¢ã€‚

### 3.3 é«˜æ•ˆè½¬æ¢å®ç°

#### æ–¹æ³•ä¸€ï¼šOpenCV å†…ç½®å‡½æ•°(æ¨è)

```cpp
std::vector<float> matToTensor(const cv::Mat& image) {
    // 1. è°ƒæ•´å°ºå¯¸
    cv::Mat resized;
    cv::resize(image, resized, cv::Size(224, 224));
    
    // 2. è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶å½’ä¸€åŒ–
    cv::Mat floatImage;
    resized.convertTo(floatImage, CV_32FC3, 1.0 / 255.0);
    
    // 3. HWC -> CHW è½¬æ¢
    std::vector<cv::Mat> channels(3);
    cv::split(floatImage, channels);  // åˆ†ç¦»é€šé“
    
    // 4. æ‹¼æ¥ä¸ºè¿ç»­å†…å­˜
    std::vector<float> result;
    result.reserve(3 * 224 * 224);
    
    for (int c = 0; c < 3; ++c) {
        result.insert(
            result.end(),
            (float*)channels[c].data,
            (float*)channels[c].data + 224 * 224
        );
    }
    
    return result;
}
```

**æ€§èƒ½åˆ†æ**ï¼š
- `cv::split()` ä¼šåˆ†é…ä¸´æ—¶å†…å­˜,æœ‰ä¸€å®šå¼€é”€
- é€‚åˆä¸­ç­‰è§„æ¨¡å›¾åƒ(224Ã—224 ~ 640Ã—640)

#### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨è½¬æ¢(æè‡´æ€§èƒ½)

```cpp
std::vector<float> matToTensorOptimized(const cv::Mat& image) {
    cv::Mat resized;
    cv::resize(image, resized, cv::Size(224, 224));
    
    const int H = 224, W = 224, C = 3;
    std::vector<float> result(C * H * W);
    
    // ç›´æ¥é‡æ’å†…å­˜å¸ƒå±€
    for (int h = 0; h < H; ++h) {
        const uchar* row = resized.ptr<uchar>(h);  // è·å–è¡ŒæŒ‡é’ˆ
        for (int w = 0; w < W; ++w) {
            for (int c = 0; c < C; ++c) {
                // HWC: row[w*C + c]
                // CHW: result[c*H*W + h*W + w]
                result[c * H * W + h * W + w] = row[w * C + c] / 255.0f;
            }
        }
    }
    
    return result;
}
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| æ–¹æ³• | 224Ã—224 | 640Ã—640 | å†…å­˜åˆ†é… |
|:----:|:-------:|:-------:|:--------:|
| OpenCV split | ~0.8ms | ~6ms | 3 æ¬¡ä¸´æ—¶ Mat |
| æ‰‹åŠ¨è½¬æ¢ | ~0.5ms | ~4ms | 0 æ¬¡ |

> [!TIP]
> å¯¹äºé«˜é¢‘è°ƒç”¨çš„åœºæ™¯(å¦‚æ¯ç§’å¤„ç† 30 å¸§),æ‰‹åŠ¨è½¬æ¢å¯èŠ‚çœçº¦ 40% çš„é¢„å¤„ç†æ—¶é—´ã€‚

### 3.4 é›¶æ‹·è´ä¼˜åŒ–æŠ€æœ¯

#### é—®é¢˜åˆ†æ

æ ‡å‡†æµç¨‹ä¸­çš„å†…å­˜æ‹·è´æ¬¡æ•°ï¼š

```mermaid
flowchart LR
    A[åŸå§‹å›¾åƒ\nMat] -->|æ‹·è´1| B[Resizeå\nMat]
    B -->|æ‹·è´2| C[Floatè½¬æ¢\nMat]
    C -->|æ‹·è´3| D[é‡æ’å\nvector]
    D -->|æ‹·è´4| E[ONNX Tensor]
    
    style A fill:#ffcdd2
    style E fill:#c8e6c9
```

**æ€»æ‹·è´æ¬¡æ•°**ï¼š4 æ¬¡å®Œæ•´å›¾åƒæ‹·è´

#### ä¼˜åŒ–æ–¹æ¡ˆï¼šç›´æ¥å†™å…¥ Tensor

```cpp
Ort::Value createTensorFromMat(
    const cv::Mat& image,
    Ort::MemoryInfo& memoryInfo
) {
    // 1. é¢„åˆ†é… Tensor å†…å­˜
    std::vector<int64_t> shape = {1, 3, 224, 224};
    size_t tensorSize = 1 * 3 * 224 * 224;
    
    // ä½¿ç”¨ unique_ptr ç®¡ç†å†…å­˜,é¿å…æå‰é‡Šæ”¾
    auto tensorData = std::make_unique<float[]>(tensorSize);
    
    // 2. Resize å¹¶ç›´æ¥å†™å…¥ Tensor
    cv::Mat resized;
    cv::resize(image, resized, cv::Size(224, 224));
    
    // 3. æ‰‹åŠ¨è½¬æ¢å¹¶å†™å…¥
    for (int h = 0; h < 224; ++h) {
        const uchar* row = resized.ptr<uchar>(h);
        for (int w = 0; w < 224; ++w) {
            for (int c = 0; c < 3; ++c) {
                tensorData[c * 224 * 224 + h * 224 + w] = 
                    row[w * 3 + c] / 255.0f;
            }
        }
    }
    
    // 4. åˆ›å»º Tensor(ä¸æ‹·è´,ç›´æ¥ä½¿ç”¨ tensorData)
    return Ort::Value::CreateTensor<float>(
        memoryInfo,
        tensorData.release(),  // è½¬ç§»æ‰€æœ‰æƒ
        tensorSize,
        shape.data(),
        shape.size()
    );
}
```

> [!CAUTION]
> ä½¿ç”¨ `release()` è½¬ç§»æ‰€æœ‰æƒå,å¿…é¡»ç¡®ä¿ Tensor çš„ç”Ÿå‘½å‘¨æœŸå†…å†…å­˜ä¸è¢«é‡Šæ”¾ã€‚å»ºè®®ä½¿ç”¨ `std::shared_ptr` æˆ–è‡ªå®šä¹‰ Deleterã€‚

---

## å››ã€å®Œæ•´å®ç°ç¤ºä¾‹

### 4.1 ModelManager å®Œæ•´ä»£ç 

```cpp
#pragma once
#include <onnxruntime_cxx_api.h>
#include <opencv2/opencv.hpp>
#include <memory>
#include <unordered_map>
#include <mutex>
#include <stdexcept>

class ModelManager {
public:
    static ModelManager& getInstance() {
        static ModelManager instance;
        return instance;
    }
    
    // åŠ è½½æ¨¡å‹
    void loadModel(const std::string& name, const std::string& path) {
        std::lock_guard<std::mutex> lock(mutex_);
        
        if (sessions_.find(name) != sessions_.end()) {
            std::cout << "[ModelManager] Model already loaded: " << name << std::endl;
            return;
        }
        
        Ort::SessionOptions options;
        options.SetIntraOpNumThreads(4);
        options.SetGraphOptimizationLevel(
            GraphOptimizationLevel::ORT_ENABLE_ALL
        );
        
        sessions_[name] = std::make_unique<Ort::Session>(
            *env_, path.c_str(), options
        );
        
        std::cout << "[ModelManager] Loaded model: " << name << std::endl;
    }
    
    // è·å–æ¨¡å‹ä¼šè¯
    Ort::Session* getSession(const std::string& name) {
        std::lock_guard<std::mutex> lock(mutex_);
        
        auto it = sessions_.find(name);
        if (it == sessions_.end()) {
            throw std::runtime_error("Model not found: " + name);
        }
        
        return it->second.get();
    }
    
    // é¢„çƒ­æ‰€æœ‰æ¨¡å‹
    void warmUpAll() {
        std::lock_guard<std::mutex> lock(mutex_);
        
        for (auto& [name, session] : sessions_) {
            warmUpModel(name, session.get());
        }
    }
    
    ModelManager(const ModelManager&) = delete;
    ModelManager& operator=(const ModelManager&) = delete;
    
private:
    ModelManager() {
        env_ = std::make_unique<Ort::Env>(
            ORT_LOGGING_LEVEL_WARNING, "ModelManager"
        );
    }
    
    void warmUpModel(const std::string& name, Ort::Session* session) {
        auto inputShape = session->GetInputTypeInfo(0)
            .GetTensorTypeAndShapeInfo()
            .GetShape();
        
        size_t inputSize = 1;
        for (auto dim : inputShape) {
            inputSize *= (dim > 0 ? dim : 1);
        }
        
        std::vector<float> dummyInput(inputSize, 0.5f);
        
        auto memoryInfo = Ort::MemoryInfo::CreateCpu(
            OrtArenaAllocator, OrtMemTypeDefault
        );
        
        Ort::Value inputTensor = Ort::Value::CreateTensor<float>(
            memoryInfo,
            dummyInput.data(),
            dummyInput.size(),
            inputShape.data(),
            inputShape.size()
        );
        
        auto inputName = session->GetInputNameAllocated(0, Ort::AllocatorWithDefaultOptions());
        auto outputName = session->GetOutputNameAllocated(0, Ort::AllocatorWithDefaultOptions());
        
        const char* inputNames[] = {inputName.get()};
        const char* outputNames[] = {outputName.get()};
        
        session->Run(
            Ort::RunOptions{nullptr},
            inputNames, &inputTensor, 1,
            outputNames, 1
        );
        
        std::cout << "[ModelManager] Warmed up: " << name << std::endl;
    }
    
    std::unique_ptr<Ort::Env> env_;
    std::unordered_map<std::string, std::unique_ptr<Ort::Session>> sessions_;
    std::mutex mutex_;
};
```

### 4.2 ä½¿ç”¨ç¤ºä¾‹

```cpp
int main() {
    // 1. è·å– ModelManager å®ä¾‹
    auto& manager = ModelManager::getInstance();
    
    // 2. åŠ è½½æ‰€æœ‰æ¨¡å‹
    manager.loadModel("mobilenet", "models/mobilenetv3_small.onnx");
    manager.loadModel("yolov8n", "models/yolov8n.onnx");
    
    // 3. é¢„çƒ­æ¨¡å‹
    manager.warmUpAll();
    
    // 4. åœ¨æ£€æµ‹å™¨ä¸­ä½¿ç”¨
    auto* session = manager.getSession("mobilenet");
    
    // 5. æ‰§è¡Œæ¨ç†
    cv::Mat image = cv::imread("test.jpg");
    auto inputTensor = matToTensor(image);
    
    // ... æ¨ç†ä»£ç  ...
    
    return 0;
}
```

---

## äº”ã€æ€§èƒ½åŸºå‡†æµ‹è¯•

### 5.1 æ¨¡å‹åŠ è½½æ—¶é—´

| æ¨¡å‹ | æ–‡ä»¶å¤§å° | åŠ è½½æ—¶é—´ | é¢„çƒ­æ—¶é—´ |
|:----:|:--------:|:--------:|:--------:|
| MobileNetV3-Small | 2.5 MB | ~50ms | ~15ms |
| YOLOv8n | 6.3 MB | ~120ms | ~30ms |
| PP-OCRv4 Det | 4.4 MB | ~80ms | ~25ms |
| PP-OCRv4 Rec | 10.5 MB | ~200ms | ~40ms |

**æ€»å¯åŠ¨æ—¶é—´**ï¼šçº¦ 450ms (åŠ è½½) + 110ms (é¢„çƒ­) = **560ms**

> [!TIP]
> å¯ä»¥åœ¨åå°çº¿ç¨‹ä¸­å¹¶è¡ŒåŠ è½½æ¨¡å‹,è¿›ä¸€æ­¥ç¼©çŸ­å¯åŠ¨æ—¶é—´è‡³çº¦ 250msã€‚

### 5.2 æ•°æ®è½¬æ¢æ€§èƒ½

æµ‹è¯•ç¯å¢ƒï¼šIntel i7-10700K, å•çº¿ç¨‹

| æ“ä½œ | 224Ã—224 | 640Ã—640 | 1920Ã—1080 |
|:----:|:-------:|:-------:|:---------:|
| cv::resize | 0.3ms | 2.5ms | 8ms |
| HWCâ†’CHW (OpenCV) | 0.5ms | 3.5ms | 12ms |
| HWCâ†’CHW (æ‰‹åŠ¨) | 0.3ms | 2ms | 7ms |
| **æ€»è®¡(æ‰‹åŠ¨)** | **0.6ms** | **4.5ms** | **15ms** |

---

## å…­ã€å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: ONNX Runtime æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶

**é”™è¯¯ä¿¡æ¯**ï¼š
```
Error: Model file not found: models/mobilenet.onnx
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®(ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºå¯æ‰§è¡Œæ–‡ä»¶çš„è·¯å¾„)
2. ç¡®è®¤æ–‡ä»¶æ‰©å±•åä¸º `.onnx`
3. æ£€æŸ¥æ–‡ä»¶æƒé™

### Q2: æ¨ç†ç»“æœå…¨ä¸º NaN

**å¯èƒ½åŸå› **ï¼š
1. è¾“å…¥æ•°æ®æœªå½’ä¸€åŒ–(åº”åœ¨ [0, 1] æˆ– [-1, 1] èŒƒå›´)
2. è¾“å…¥å½¢çŠ¶ä¸åŒ¹é…
3. æ¨¡å‹æ–‡ä»¶æŸå

**è°ƒè¯•æ–¹æ³•**ï¼š
```cpp
// æ‰“å°è¾“å…¥æ•°æ®èŒƒå›´
float minVal = *std::min_element(inputData.begin(), inputData.end());
float maxVal = *std::max_element(inputData.begin(), inputData.end());
std::cout << "Input range: [" << minVal << ", " << maxVal << "]" << std::endl;

// æ‰“å°è¾“å…¥å½¢çŠ¶
for (auto dim : inputShape) {
    std::cout << dim << " ";
}
std::cout << std::endl;
```

### Q3: å†…å­˜æ³„æ¼

**æ£€æµ‹å·¥å…·**ï¼š
- Windows: Visual Studio Memory Profiler
- Linux: Valgrind

**å¸¸è§åŸå› **ï¼š
1. å¿˜è®°é‡Šæ”¾ `Ort::Value` ä¸­çš„è‡ªå®šä¹‰å†…å­˜
2. å¾ªç¯ä¸­é‡å¤åˆ›å»º Session

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨æ™ºèƒ½æŒ‡é’ˆ(`std::unique_ptr`, `std::shared_ptr`)
- ç¡®ä¿ Session åœ¨æ•´ä¸ªåº”ç”¨ç”Ÿå‘½å‘¨æœŸå†…åªåˆ›å»ºä¸€æ¬¡

---

## ä¸ƒã€ä¸‹ä¸€æ­¥è®¡åˆ’

å®Œæˆæœ¬é˜¶æ®µå,æ‚¨å°†æ‹¥æœ‰ï¼š

âœ… ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†æ¥å£  
âœ… é«˜æ•ˆçš„æ•°æ®è½¬æ¢å·¥å…·  
âœ… ç¨³å®šçš„æ¨ç†åŸºç¡€è®¾æ–½  

**ä¸‹ä¸€é˜¶æ®µ**ï¼š[ç¬¬äºŒé˜¶æ®µ - è§£ç æµæ°´çº¿å¼€å‘](phase2_decoder_pipeline.md)

---

## å‚è€ƒèµ„æ–™

- [ONNX Runtime å®˜æ–¹æ–‡æ¡£](https://onnxruntime.ai/docs/)
- [PaddleInference C++ API](https://www.paddlepaddle.org.cn/inference/master/api_reference/cxx_api_index.html)
- [OpenCV Mat ç±»å‚è€ƒ](https://docs.opencv.org/4.x/d3/d63/classcv_1_1Mat.html)
- [C++ å¹¶å‘ç¼–ç¨‹å®æˆ˜](https://www.manning.com/books/c-plus-plus-concurrency-in-action-second-edition)
